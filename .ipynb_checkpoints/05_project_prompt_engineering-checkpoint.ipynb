{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f193828c-cca1-422f-a0a0-f27896ca3bd1",
   "metadata": {},
   "source": [
    "# Project : Prompt Engineering\n",
    "\n",
    "\n",
    "The quality of the instructions you give to an LLM can have a large effect on the quality of its outputs, especially for complex tasks. This project related to prompt design will help you learn how to craft prompts that produce accurate and consistent results.\n",
    "\n",
    "The project will be based on : \n",
    "- https://docs.anthropic.com/claude/docs/introduction-to-prompt-design\n",
    "\n",
    "- https://docs.google.com/spreadsheets/d/19jzLgRruG9kjUQNKtCg1ZjdD6l6weA6qRXG5zLIAhC8/edit#gid=150872633\n",
    "\n",
    "However, we will use Cohere and not Claude. It means, that you need to retain the principles of prompt engineering  but not necessarily exactly the same syntax described in the documents\n",
    "\n",
    "Moreover, you can check :\n",
    "- https://python.langchain.com/docs/modules/model_io/prompts/quick_start\n",
    "- https://docs.google.com/presentation/d/1zxkSI7lLUBrZycA-_znwqu8DDyVhHLkQGScvzaZrUns/edit?pli=1#slide=id.g2accb454d71_79_175 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c99f7ec-64c8-415c-a8e9-33c8042bb26d",
   "metadata": {},
   "source": [
    "### Complete pre-requisite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff372e2-dbf4-476c-bee7-09d551d43883",
   "metadata": {},
   "source": [
    "#### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60530a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "# You can add other package here if needed\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efe4eba-f301-4f50-9260-a7ddb778afac",
   "metadata": {},
   "source": [
    "#### Initialize LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36fd57c-ec76-49e6-95ae-368e836725ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Cohere\n",
    "\n",
    "llm = Cohere(temperature=0.75, cohere_api_key=os.environ.get('COHERE_API_KEY'))\n",
    "\n",
    "print(llm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e771f89a-0a87-4b68-9cd4-6c8e481a89ea",
   "metadata": {},
   "source": [
    "## Prompt Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1370c0b7",
   "metadata": {},
   "source": [
    "### Being Clear and Direct\n",
    "\n",
    "From : https://docs.google.com/spreadsheets/d/19jzLgRruG9kjUQNKtCg1ZjdD6l6weA6qRXG5zLIAhC8/edit#gid=1733615301\n",
    "\n",
    "\n",
    "**LLM responds best to clear and direct instructions.** \n",
    "\n",
    "Think of llm like any other human that is new to the job. Claude has no context on what to do aside from what you literally tell it. Just as when you instruct a human for the first time on a task, the more you explain exactly what you want in a straightforward manner to LLM, the better and more accurate LLM's response will be.\n",
    "\n",
    "**The Golden Rule of Clear Prompting** : show your prompt to a friend and ask them if they could follow the instructions themselves and produce the exact result you want. If they're confused, any LLM will be confused as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c797b8-9db8-4f1c-8d3d-bf856565b68b",
   "metadata": {},
   "source": [
    "#### Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77638288-af34-4103-9f09-7ca0c7491dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# adapt the prompt to make the llm outpout its answer in Spanish\n",
    "basic_prompt = \"Write a haiku about robots.\"\n",
    "# improved_prompt = ?\n",
    "llm.invoke(improved_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902679cb-f1dd-4daf-a584-3066742c7ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Modify the prompt so that the llm doesn't equivocate at all and responds with ONLY the name of one specific player, with no other words or punctuation. \n",
    "basic_prompt = \"Who is the best basketball player of all time? Please choose one specific player.\"\n",
    "# improved_prompt = ?\n",
    "llm.invoke(improved_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec2110f-677f-4f48-97da-4443515305c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Modify the prompt so that the llm doesn't equivocate at all and responds with ONLY the name of one specific player, with no other words or punctuation. \n",
    "basic_prompt = \"Who is the best basketball player of all time? Please choose one specific player.\"\n",
    "# improved_prompt = ?\n",
    "llm.invoke(improved_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a537a81-5550-4798-af4d-ac122ca26e36",
   "metadata": {},
   "source": [
    "### Assigning Roles (Role Prompting)\n",
    "\n",
    " From : https://docs.google.com/spreadsheets/d/19jzLgRruG9kjUQNKtCg1ZjdD6l6weA6qRXG5zLIAhC8/edit#gid=2055375080\n",
    " \t\n",
    "Continuing on the theme of LLM having no context aside from what you say, it's sometimes important to prompt llm to inhabit a specific role (including all necessary context). This is also known as role prompting. The more detail to the role context, the better.\n",
    "\n",
    "Priming an llm with a role can improve the performance in a variety of fields, from writing to coding to summarizing. It's like how humans can sometimes be helped when told to \"think like a ______\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1808ae3-6388-460f-b431-55ed46833d3d",
   "metadata": {},
   "source": [
    "#### Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8f39d6-8638-424e-813d-bde910f3f910",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generate a better output by giving a role\n",
    "basic_prompt = \"Explains what is a republic.\"\n",
    "# improved_prompt = ?\n",
    "llm.invoke(improved_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2877c34-de6b-4d7a-9e69-89092422c9e3",
   "metadata": {},
   "source": [
    "### Separating Data from Instructions\n",
    "\n",
    "Not able to provide an exercice : https://docs.google.com/spreadsheets/d/19jzLgRruG9kjUQNKtCg1ZjdD6l6weA6qRXG5zLIAhC8/edit#gid=1519813817"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b6171a-0805-4cdd-90bf-85e91b09e9a6",
   "metadata": {},
   "source": [
    "### Formatting Output\n",
    "\n",
    "From: https://docs.google.com/spreadsheets/d/19jzLgRruG9kjUQNKtCg1ZjdD6l6weA6qRXG5zLIAhC8/edit#gid=257656347 \n",
    "\n",
    "An LLM can format its output in a wide variety of ways. You just need to ask for it to do so!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc21042-4c73-4ce1-b797-d8fff0902b71",
   "metadata": {},
   "source": [
    "#### Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e3342e-2787-4f79-9103-9399b2ede8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Forced to make a choice, Cohere designates Michael Jordan as the best basketball player of all time. Can we get Cohere to pick someone else?\n",
    "basic_prompt = \"Who is the best basketball player of all time?\"\n",
    "# improved_prompt = ?\n",
    "llm.invoke(improved_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0f4824-34ff-4225-a745-96eccaa12bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instead of receiving a basic output, you want to ouptut in a table format\n",
    "basic_prompt = \"Gives the main events of France.\"\n",
    "# improved_prompt = ?\n",
    "llm.invoke(improved_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f8eb26-0277-4622-a858-cd3c292d82f7",
   "metadata": {},
   "source": [
    "### Thinking Step by Step \n",
    "\n",
    "From: https://docs.google.com/spreadsheets/d/19jzLgRruG9kjUQNKtCg1ZjdD6l6weA6qRXG5zLIAhC8/edit#gid=1213680236\n",
    "\n",
    " \t\n",
    "If someone woke you up and immediately started asking you several complicated questions that you had to respond to right away, how would you do? Probably not as good as if you were given time to think through your answer first. \n",
    "\n",
    "Guess what? An LLM is the same way.\n",
    "\n",
    "Giving Cohere time to think step by step sometimes makes Cohere more accurate, particularly for complex tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3938c76-4e49-44a6-b68d-24a387c833f2",
   "metadata": {},
   "source": [
    "#### Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5eec2b-5016-426f-9f5a-1d377cddb284",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Find something where you need to  use the \"Think step by step method\"\n",
    "basic_prompt = \"\"\n",
    "# improved_prompt = ?\n",
    "llm.invoke(basic_prompt)\n",
    "llm.invoke(improved_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0508c821-2a96-4fb0-a7eb-e2d39014851b",
   "metadata": {},
   "source": [
    "### Using Examples\n",
    "\n",
    "Check this sheet to understand the difference with \"Formatting output\": https://docs.google.com/spreadsheets/d/19jzLgRruG9kjUQNKtCg1ZjdD6l6weA6qRXG5zLIAhC8/edit#gid=1640903723\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a6fdc2-e790-43e6-ac2f-30ef6a5ad22d",
   "metadata": {},
   "source": [
    "#### Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0649f48b-f151-496c-9f3f-b456c9dca192",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Find something where you need to  use the \"Using Examples\"\n",
    "# The goal is to understanding how it is powerful compared to \"Formatting output\"\n",
    "# When you exactly know what you want\n",
    "basic_prompt = \"\"\n",
    "# improved_prompt = ?\n",
    "llm.invoke(basic_prompt)\n",
    "llm.invoke(improved_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4652da-77b5-48db-b3b1-85ce26ee8662",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
